# Multimodal Visual Question Answering with Amazon Berkeley Objects Dataset

# Visual Recognition Mini Project-2
## Contributors: 
<h3 align = "center">
Varsha Yamsani(IMT2022506): Yamsani.Varsha@iiitb.ac.in<br> 
R Harshavardhan(IMT2022515): R.Harshavardhan@iiitb.ac.in<br>
Keshav Goyal(IMT2022560) Keshav.Goyal560@iiitb.ac.in</h3>

# Introduction

This assignment involves creating a multiple-choice Visual Question Answering (VQA)
dataset using the Amazon Berkeley Objects (ABO) dataset, evaluating baseline models,
fine-tuning using Low-Rank Adaptation (LoRA), and assessing performance using standard metrics.

### Report

Please refer to the report [`IMT2022506_515_560_MiniProject2.pdf`](./IMT2022506_515_560_MiniProject2.pdf) located in the root directory of this repository for detailed analysis, methodology, and results.

---

### Inference Instructions

To run the inference:

1. Download the [`sample-submission`](./sample-submission) folder.
2. Set up a Python virtual environment and install required dependencies for your `run_inference_for_all.py` first.


